"""-----------------------------------------------------------------------------
Name: attribute_grid_comparison.py
Purpose: Compares tables with the same name in a given polygon.  The tool
        will examin the total rows and compare the field's total differences
        and generate a set of rankings in that polygon grid for the fields.
Description: Using a polygon feature class, a generic attribute comparison is
        performed on non-spatial data. Each cell is given a weight based on the
        attribute difference count. The comparison looks at attibute
        differences for each area in the polygon grid. The tool does not know
        what the correct value should be, only that a NULL value is incorrect
        and a non-NULL value is correct. A weight is then generated by
        comparing the areas total score verse the expected. The weight is then
        further generalized down into a ranking from -5 to 5, where 5 is where
        attribute completeness increased significantly and -5 attribute
        completeness decreased.
Requirements: Python 2.7.x/Python3.x, ArcGIS 10.4+/Pro 1.2+
Author(s): Andrew Chapkowski, Contractor for National Geospatial-Intelligence
        Agency (NGA) | Gregory Brunner, Contractor for NGA
Program Manager: Derek Silva, NGA (Derek.A.Silva@nga.mil)
Created: December, 2016
Modified By: Gregory Brunner, Contractor for NGA
Copyright: Esri
License:
-----------------------------------------------------------------------------"""
import os
import sys
import platform
import itertools

import arcpy
from arcpy import env
from arcpy import da
import numpy as np
import pandas as pd
#--------------------------------------------------------------------------
class FunctionError(Exception):
    """ raised when a function fails to run """
    pass
#--------------------------------------------------------------------------
def trace():
    """
        trace finds the line, the filename
        and error message and returns it
        to the user
    """
    import traceback
    tb = sys.exc_info()[2]
    tbinfo = traceback.format_tb(tb)[0]
    # script name + line number
    line = tbinfo.split(", ")[1]
    # Get Python syntax error
    #
    synerror = traceback.format_exc().splitlines()[-1]
    return line, __file__, synerror
#--------------------------------------------------------------------------
def grouper_it(n, iterable):
    """
    creates chunks of cursor row objects to make the memory
    footprint more manageable
    """
    it = iter(iterable)
    while True:
        chunk_it = itertools.islice(it, n)
        try:
            first_el = next(chunk_it)
        except StopIteration:
            return
        yield itertools.chain((first_el,), chunk_it)
#--------------------------------------------------------------------------
def calc_chunk_size():
    """determines the chunk size based on 32 vs 64-bit python"""
    try:
        if platform.architecture()[0].lower() == "32bit":
            return 50000
        else:
            return 500000
    except:
        line, filename, synerror = trace()
        raise FunctionError(
                {
                    "function": "calc_chunk_size",
                    "line": line,
                    "filename": __file__,
                    "synerror": synerror,
                    "arc" : str(arcpy.GetMessages(2))
                }
        )
def replace_null_values(fc,
                        fields="*",
                        oid_field=None,
                        null_value=0,
                        where_clause=None):
    """updates a set of rows in chunks

    """
    if fields is None or \
       isinstance(fields, list) == False or \
       fields == "*":
        fields = [field.name for field in arcpy.ListFields(fc) \
                  if field.type not in ('Geometry', 'Blob', 'Raster')]
    if oid_field is None:
        oid_field = arcpy.Describe(fc).OIDFieldName
    chunk_size = calc_chunk_size()
    if oid_field not in fields:
        fields.append(oid_field)
    with da.SearchCursor(fc, fields,
                         where_clause=where_clause) as cursor:
        search_fields = [field for field in cursor.fields if field != oid_field]
        for group in grouper_it(chunk_size, cursor):
            df = pd.DataFrame.from_records(group, columns=cursor.fields)
            for field in search_fields:
                df.loc[df[field].isnull(), field] = null_value
                del field
            array = df.to_records(index=False)
            da.ExtendTable(fc, oid_field,
                           array, oid_field,
                           False)
            del array
    return fc
#--------------------------------------------------------------------------
def calculate_nulls(fc, fields):
    """
    summarizes the number of NULL/None values in a given set of rows
    """
    try:
        oid = arcpy.Describe(fc).OIDFieldName
        if oid not in fields:
            fields.append(oid)
        chunk_size = calc_chunk_size()
        with da.SearchCursor(fc, fields) as cursor:
            for group in grouper_it(chunk_size, cursor):
                df = pd.DataFrame.from_records(group, columns=cursor.fields)
                df['NULL_COUNT'] = len(df.columns) - df.count(axis=1)
                df['PERCENT_COMP'] = 100 - (100 * (df['NULL_COUNT'] / (len(fields) -1)))
                df['RANKING'] = -1
                df.loc[(df['PERCENT_COMP'] >= 0.0) & (df['PERCENT_COMP'] <= 20), 'RANKING'] = 1
                df.loc[(df['PERCENT_COMP'] > 20) & (df['PERCENT_COMP'] <= 40), 'RANKING'] = 2
                df.loc[(df['PERCENT_COMP'] > 40) & (df['PERCENT_COMP'] <= 60), 'RANKING'] = 3
                df.loc[(df['PERCENT_COMP'] > 60) & (df['PERCENT_COMP'] <= 80), 'RANKING'] = 4
                df.loc[(df['PERCENT_COMP'] > 80), 'RANKING'] = 5
                columns=[oid, "NULL_COUNT", "PERCENT_COMP", "RANKING"]
                copy_df = df[columns].copy()
                arcpy.da.ExtendTable(fc,
                                     oid,
                                     copy_df.to_records(index=False),
                                     oid,
                                     False)
        return fc
    except:
        line, filename, synerror = trace()
        raise FunctionError(
                {
                "function": "calculate_nulls",
                "line": line,
                "filename": filename,
                "synerror": synerror,
                "arc" : str(arcpy.GetMessages(2))
                }
                )
#--------------------------------------------------------------------------
def main(*argv):
    """ main driver of program """
    try:
        old_fc = argv[0]
        new_fc = argv[1]
        polygon_grid = argv[2]
        fields = str(argv[3]).split(';')#argv[3]
        out_gdb = argv[4]
        #   Local Variables
        #
        scratchGDB = env.scratchGDB
        scratchFolder = env.scratchFolder
        copy_grid = os.path.join(out_gdb, "grid")
        copy_old_fc = os.path.join(scratchGDB, os.path.basename(old_fc) + "_old")
        copy_new_fc = os.path.join(scratchGDB, os.path.basename(new_fc) + "_new")
        intOld = os.path.join(scratchGDB, "intold")
        intNew = os.path.join(scratchGDB, "intnew")
        sumOld = os.path.join(scratchGDB, "sum_old")
        sumNew = os.path.join(scratchGDB, "sum_new")
        temp_csv = os.path.join(scratchFolder, "temp_csv.csv")
        #  Logic
        #
        #  Validate that fields exist in both tables
        old_fc_flds = {field.name for field in arcpy.ListFields(old_fc) if field.name in fields}
        new_fc_flds = {field.name for field in arcpy.ListFields(new_fc) if field.name in fields}
        if len(new_fc_flds) != len(old_fc_flds):
            missing = list(old_fc_flds-new_fc_flds) + list(new_fc_flds-old_fc_flds)

            arcpy.AddWarning("Comparison Fields Missing: %s" % ",".join(missing))
            fields = [fields.remove(v) for v in missing if v in fields]
        if len(fields) == 0:
            raise Exception("All fields provided do not exist in each dataset.  Nothing to compare.")
        #copy the data
        copy_grid = arcpy.CopyFeatures_management(polygon_grid, copy_grid)[0]
        copy_old_fc = arcpy.FeatureClassToFeatureClass_conversion(old_fc,
                                                                  out_path=os.path.dirname(copy_old_fc),
                                                                  out_name=os.path.basename(copy_old_fc))[0]
        copy_new_fc = arcpy.FeatureClassToFeatureClass_conversion(new_fc,
                                                                  out_path=os.path.dirname(copy_new_fc),
                                                                  out_name=os.path.basename(copy_new_fc))[0]

        # get the null counts
        oldResult = calculate_nulls(copy_old_fc, fields)
        newResult = calculate_nulls(copy_new_fc, fields)
        #  Intersect Polygon with old/new
        intOld = arcpy.Intersect_analysis(in_features=[copy_grid, copy_old_fc],
                                          out_feature_class=intOld,
                                          join_attributes="ALL",
                                          cluster_tolerance="-1 Unknown",
                                          output_type="INPUT")[0]
        intNew = arcpy.Intersect_analysis(in_features=[copy_grid, copy_new_fc],
                                          out_feature_class=intNew,
                                          join_attributes="ALL",
                                          cluster_tolerance="-1 Unknown",
                                          output_type="INPUT")[0]
        #  Aggregavte and Average out ranking by grid OID
        case_field = "FID_%s" % os.path.basename(copy_grid)
        sumOld = arcpy.Statistics_analysis(in_table=intOld,
                                  out_table=sumOld,
                                  statistics_fields="NULL_COUNT SUM;PERCENT_COMP MEAN;RANKING MEAN",
                                  case_field=case_field)[0]
        sumNew = arcpy.Statistics_analysis(in_table=intNew,
                                  out_table=sumNew,
                                  statistics_fields="NULL_COUNT SUM;PERCENT_COMP MEAN;RANKING MEAN",
                                  case_field=case_field)[0]
        array_old = da.TableToNumPyArray(sumOld, [case_field, 'SUM_NULL_COUNT', 'MEAN_PERCENT_COMP', 'MEAN_RANKING'])
        dtype_old = [('FID_grid', '<i4'), ('NULL_COUNT_OLD', '<f8'), ('PERCENT_COMP_OLD', '<f8'), ('RANKING_OLD', '<f8')]
        array_old.dtype = dtype_old
        array_new = da.TableToNumPyArray(sumNew, [case_field, 'SUM_NULL_COUNT', 'MEAN_PERCENT_COMP', 'MEAN_RANKING'])
        dtype_new = [('FID_grid', '<i4'), ('NULL_COUNT_NEW', '<f8'), ('PERCENT_COMP_NEW', '<f8'), ('RANKING_NEW', '<f8')]
        array_new.dtype = dtype_new
        # Join stats table back to grid
        df_old = pd.DataFrame(data=array_old, columns=array_old.dtype.fields.keys())
        df_new = pd.DataFrame(data=array_new, columns=array_new.dtype.fields.keys())
        join_df = df_new.set_index('FID_grid').join(df_old.set_index('FID_grid'))
        join_df['FID_grid'] = join_df.index
        dtype_join = list(set(dtype_new + dtype_old))
        join_df.loc[(join_df['RANKING_OLD'].isnull()), 'RANKING_OLD'] = 0
        join_df.loc[(join_df['RANKING_NEW'].isnull()), 'RANKING_NEW'] = 0
        join_df['DIFF_RANKING'] = join_df['RANKING_NEW'] - join_df['RANKING_OLD']
        if os.path.isfile(temp_csv):
            os.remove(temp_csv)
        join_df.to_csv(temp_csv, columns=[case_field, 'RANKING_OLD',
                                          'RANKING_NEW','DIFF_RANKING'])
        array = da.TableToNumPyArray(temp_csv, [case_field, 'RANKING_OLD',
                                                'RANKING_NEW','DIFF_RANKING'])
        oid = arcpy.Describe(copy_grid).OIDFieldName
        da.ExtendTable(copy_grid,
                       oid,
                       array,
                       case_field,
                       False)
        with arcpy.da.UpdateCursor(copy_grid,
                                   ['DIFF_RANKING','RANKING_OLD','RANKING_NEW'],
                                   where_clause="DIFF_RANKING IS NULL or RANKING_OLD is NULL or RANKING_NEW is NULL") as urows:
            for urow in urows:
                if urow[0] is None:
                    urow[0] = 0
                if urow[1] is None:
                    urow[1] = 0
                if urow[2] is None:
                    urow[2] = 0
                #urow[0] = 0
                urows.updateRow(urow)
                del urow
            del urows
        if os.path.isfile(temp_csv):
            os.remove(temp_csv)
        #  Output
        #
        arcpy.SetParameterAsText(5, copy_grid)
    except arcpy.ExecuteError:
        line, filename, synerror = trace()
        arcpy.AddError("error on line: %s" % line)
        arcpy.AddError("error in file name: %s" % filename)
        arcpy.AddError("with error message: %s" % synerror)
        arcpy.AddError("ArcPy Error Message: %s" % arcpy.GetMessages(2))
    except FunctionError as f_e:
        messages = f_e.args[0]
        arcpy.AddError("error in function: %s" % messages["function"])
        arcpy.AddError("error on line: %s" % messages["line"])
        arcpy.AddError("error in file name: %s" % messages["filename"])
        arcpy.AddError("with error message: %s" % messages["synerror"])
        arcpy.AddError("ArcPy Error Message: %s" % messages["arc"])
    except:
        line, filename, synerror = trace()
        arcpy.AddError("error on line: %s" % line)
        arcpy.AddError("error in file name: %s" % filename)
        arcpy.AddError("with error message: %s" % synerror)
#--------------------------------------------------------------------------
if __name__ == "__main__":
    env.overwriteOutput = True
    argv = tuple(arcpy.GetParameterAsText(i)
    for i in range(arcpy.GetArgumentCount()))
    main(*argv)
